{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding model\n",
    "def get_embedding_model(model_name=\"/teamspace/studios/this_studio/bge-small-en-v1.5\"):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "    return embed_model\n",
    "\n",
    "# generator model\n",
    "def get_llm(model_name=\"llama3-8b-8192\"):\n",
    "    llm = Groq(model=model_name, api_key=os.getenv(\"GROQ_API\"), temperature=0.8)\n",
    "    return llm\n",
    "\n",
    "## get deeplake vector database\n",
    "def get_vector_database(id, dataset_name):\n",
    "    my_activeloop_org_id = id # \"hunter\"\n",
    "    my_activeloop_dataset_name = dataset_name # \"Vietnamese-law-RAG\"\n",
    "    dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def get_index(vector_store):\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query generation / rewriting\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "query_str = \"Vượt đèn đỏ sẽ bị gì?\"\n",
    "\n",
    "prompt_for_generating_query = PromptTemplate(\n",
    "    \"\"\"Bạn là một trợ lý xuất sắc trong việc tạo ra các câu truy vấn tìm kiếm liên quan. Dựa trên câu truy vấn đầu vào dưới đây, hãy tạo ra {num_queries} truy vấn tìm kiếm liên quan, mỗi câu trên một dòng. Lưu ý, trả lời bằng tiếng Việt và chỉ trả về các truy vấn đã tạo ra.\n",
    "\n",
    "### Câu truy vấn đầu vào: {query}\n",
    "\n",
    "### Các câu truy vấn:\"\"\"\n",
    ")\n",
    "\n",
    "def generate_queries(llm, query_str, num_queries=4):\n",
    "    fmt_prompt = prompt_for_generating_query.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries =  response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "def run_queries(queries, retrievers):\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.retrieve(query))\n",
    "    \n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, tasks)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "def get_bm25_retriever(index, similarity_top_k=13625):\n",
    "    source_nodes = index.as_retriever(similarity_top_k=similarity_top_k).retrieve(\"test\")\n",
    "    nodes = [x.node for x in source_nodes]\n",
    "    bm25 = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=3)\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from vectorstor -> retrieve relevant nodes\n",
    "- feed those node into LLMs to generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://hunter/Vietnamese-law-RAG already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = get_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "llm = get_llm()\n",
    "Settings.llm = llm\n",
    "vector_store = get_vector_database(\"hunter\", \"Vietnamese-law-RAG\")\n",
    "index = get_index(vector_store=vector_store)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=3)\n",
    "bm25_retriever = get_bm25_retriever(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "vistral = Ollama(model=\"phogpt\", request_timeout=120.0)\n",
    "Settings.llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create retriever and query engine\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x7f73d7aad630>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x7f73d7aad630>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immerse yourself in the language by listening to native speakers, practicing speaking and writing, and engaging with native speakers.\n"
     ]
    }
   ],
   "source": [
    "query = \"How to be fluent foreign language faster?\"\n",
    "print(query_engine.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Câu hỏi: Theo điều 22, trong bao nhiêu ngày ph...</td>\n",
       "      <td>Câu trả lời: Phải công khai trong thời hạn ba ...</td>\n",
       "      <td>Node ID: node_9417\\nText: 5. Thời điểm công kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Câu hỏi:</td>\n",
       "      <td>- Bạn biết được khổ đường sắt là gì không?</td>\n",
       "      <td>Node ID: node_12463\\nText: 7. Công trình công ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Câu hỏi: Theo quy định, ai có thẩm quyền quyết...</td>\n",
       "      <td>Câu trả lời: Người có thẩm quyền quyết định di...</td>\n",
       "      <td>Node ID: node_9876\\nText: Vị trí mới của trạm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Câu hỏi: Theo luật, ai được công nhận có quyền...</td>\n",
       "      <td>Câu trả lời: Luật cho phép đối tượng bao gồm đ...</td>\n",
       "      <td>Node ID: node_9287\\nText: Người có quyền tự mì...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Câu hỏi: Bộ Lao động - Thương binh và Xã hội c...</td>\n",
       "      <td>Câu trả lời: Doanh nghiệp dịch vụ được miễn ph...</td>\n",
       "      <td>Node ID: node_5243\\nText: 2. Trong thời hạn 05...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Câu hỏi: Theo điều 22, trong bao nhiêu ngày ph...   \n",
       "1                                           Câu hỏi:   \n",
       "2  Câu hỏi: Theo quy định, ai có thẩm quyền quyết...   \n",
       "3  Câu hỏi: Theo luật, ai được công nhận có quyền...   \n",
       "4  Câu hỏi: Bộ Lao động - Thương binh và Xã hội c...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Câu trả lời: Phải công khai trong thời hạn ba ...   \n",
       "1         - Bạn biết được khổ đường sắt là gì không?   \n",
       "2  Câu trả lời: Người có thẩm quyền quyết định di...   \n",
       "3  Câu trả lời: Luật cho phép đối tượng bao gồm đ...   \n",
       "4  Câu trả lời: Doanh nghiệp dịch vụ được miễn ph...   \n",
       "\n",
       "                                             context  \n",
       "0  Node ID: node_9417\\nText: 5. Thời điểm công kh...  \n",
       "1  Node ID: node_12463\\nText: 7. Công trình công ...  \n",
       "2  Node ID: node_9876\\nText: Vị trí mới của trạm ...  \n",
       "3  Node ID: node_9287\\nText: Người có quyền tự mì...  \n",
       "4  Node ID: node_5243\\nText: 2. Trong thời hạn 05...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"qa_data.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.evaluation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrieverEvaluator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We can evaluate the retievers with different top_k values.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.evaluation'"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "# We can evaluate the retievers with different top_k values.\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=i)\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: node_7480\n",
      "Text: Điều 25. Trách nhiệm của cơ quan, tổ chức và người dân Thủ đô\n",
      "1. Hội đồng nhân dân,  Ủy ban nhân dân, Chủ tịch Ủy ban nhân dân các\n",
      "cấp chính quyền của  thành phố Hà Nội trong phạm vi nhiệm vụ, quyền\n",
      "hạn của mình thực hiện các quy định của pháp  luật về Thủ đô và chịu\n",
      "trách nhiệm về những vi phạm, yếu kém xảy ra trong công tác xây dựng,\n",
      "phát t...\n",
      "Score:  0.599\n",
      "\n",
      "Câu hỏi: Theo pháp luật về Thủ đô Hà Nội, thì ai có trách nhiệm giám sát việc thi hành Luật Thủ đô?\n",
      "Câu trả lời: Hội đồng nhân dân thành phố Hà Nội.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[27, 2])\n",
    "print(df.iloc[27, 0])\n",
    "print(df.iloc[27, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: node_7480\n",
      "Text: Điều 25. Trách nhiệm của cơ quan, tổ chức và người dân Thủ đô\n",
      "1. Hội đồng nhân dân,  Ủy ban nhân dân, Chủ tịch Ủy ban nhân dân các\n",
      "cấp chính quyền của  thành phố Hà Nội trong phạm vi nhiệm vụ, quyền\n",
      "hạn của mình thực hiện các quy định của pháp  luật về Thủ đô và chịu\n",
      "trách nhiệm về những vi phạm, yếu kém xảy ra trong công tác xây dựng,\n",
      "phát t...\n",
      "Score:  0.599\n",
      "\n",
      "Query: Câu hỏi: Theo pháp luật về Thủ đô Hà Nội, thì ai có trách nhiệm giám sát việc thi hành Luật Thủ đô?\n",
      "Metrics: {'mrr': 0.0, 'hit_rate': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_query = df.iloc[27, 0]\n",
    "sample_expected = embed_model.get_text_embedding(df.iloc[27, 2])\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(df.iloc[27, 2])\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PydanticOutputParser' from 'langchain_core.output_parsers' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/output_parsers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     faithfulness,\n\u001b[1;32m      3\u001b[0m     answer_relevancy,\n\u001b[1;32m      4\u001b[0m     context_precision,\n\u001b[1;32m      5\u001b[0m     context_recall,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcritique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m harmfulness\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ragas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ragas/adaptation.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\n\u001b[1;32m     11\u001b[0m     metrics: t\u001b[38;5;241m.\u001b[39mList[MetricWithLLM],\n\u001b[1;32m     12\u001b[0m     language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     13\u001b[0m     llm: t\u001b[38;5;241m.\u001b[39mOptional[BaseRagasLLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     cache_dir: t\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Adapt the metric to a different language.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ragas/metrics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_correctness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerCorrectness, answer_correctness\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_relevance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancy, answer_relevancy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity, answer_similarity\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ragas/metrics/_answer_correctness.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RagasoutputParser, get_json_format_instructions\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prompt, PromptValue\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ragas/llms/output_parser.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputParserException\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticOutputParser\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PydanticOutputParser' from 'langchain_core.output_parsers' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/output_parsers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
